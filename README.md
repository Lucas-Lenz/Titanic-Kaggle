# Titanic-Kaggle
Machine Learning Kaggle: creating a model (or models) that predicts the survival rate of certain passengers in the Titanic disaster.
********
tradu√ß√£o:
Machine Learning Kaggle: cria√ß√£o de modelo(s) que prev√™ a taxa de sobreviv√™ncia de determinados tripulantes no desastre do Titanic.

# Comments in English:

Step #1: Data Cleaning
In this step, we processed the train and test datasets, which contained empty spaces and missing values (NA). We considered three possible approaches:

1. Deleting missing data
2. Filling in missing values with the mean or median
3. Using regression to estimate the missing values

We opted to use the mean, even though this might reduce data variability. Specifically, we replaced missing age values with the mean age of women for female NA values and the mean age of men for male NA values. This choice improved prediction accuracy compared to using a single mean for both genders. Other NA replacements are detailed in the code, following a general pattern of using the majority value in the dataset or the mean of the available values.

Step #2: Random Forest Model
We tested various numbers of trees and settled on 200 trees as the optimal choice. Beyond a certain number of trees, performance stabilizes, and adding more trees does not lead to overfitting‚Äîit actually reduces variance by decorrelating the estimators. We chose mtry = 3, based on the formula sqrt(number of predictors), where sqrt(7) ‚âà 2.64, rounded to 3.

Random Forest Prediction Accuracy: 77.75%

Step #3: Decision Tree Model
We also implemented a Decision Tree, which achieved a slightly higher prediction accuracy:

Decision Tree Prediction Accuracy: 77.99%
Additionally, we performed pruning on the tree. We pruned it at the point of minimum error found in the table generated by print(tree$cptable), which was 0.01. However, the pruned tree did not significantly differ from the unpruned version, indicating that the default parameterization was already well-adjusted.

Why pruning? Pruning prevents overfitting and ensures that small variations in the data do not drastically change the tree structure. It removes overly specific lower branches, making the model more generalizable.

Model Selection and Explanation
A Decision Tree is a hierarchical model consisting of decision nodes and leaf nodes. Decision nodes split into two or more branches, while leaf nodes represent final classifications. This method provides an intuitive visualization of the decision-making process.

A Random Forest builds multiple decision trees and averages their predictions. Each tree is trained on a random subset of data, ensuring they remain uncorrelated. This reduces variance and makes the model more robust to outliers.

Comparing the Results
Our Decision Tree slightly outperformed the Random Forest model.

‚úîÔ∏è Advantages of Decision Trees:
Easy to interpret
No need for normalization or standardization
Fast classification of new data

‚ùå Disadvantages of Decision Trees:
Can become overly complex
Sensitive to small changes in data (pruning helps mitigate this)
Constructing the optimal tree is an NP-complete problem
In contrast, Random Forest has advantages in scenarios where decision trees may overfit. Since it builds multiple uncorrelated trees on random subsets, it naturally avoids overfitting in most cases.

üìå Note: We did not apply feature scaling, which could have potentially improved the Random Forest model‚Äôs performance.
